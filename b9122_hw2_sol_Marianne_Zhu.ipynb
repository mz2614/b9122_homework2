{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1\n"
      ],
      "metadata": {
        "id": "F6jbC0m6EImQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 1"
      ],
      "metadata": {
        "id": "RGVPpGzGENF3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import urllib.request\n",
        "\n",
        "# Check if the article is a press release\n",
        "def is_press(soup):\n",
        "    press_release_link = soup.find('a', href=lambda href: href and href.startswith(\"/en/press-release\"), hreflang=\"en\", text=\"Press Release\")\n",
        "    return press_release_link is not None\n",
        "\n",
        "seed_url = \"https://press.un.org/en\"\n",
        "\n",
        "urls = [seed_url]  # Queue of URLs to crawl\n",
        "press_releases = [] # Store found press releases\n",
        "\n",
        "maxNumUrl = 10  # Set the maximum number of URLs to visit\n",
        "print(\"Starting with url=\" + str(urls))\n",
        "while urls and len(press_releases) < maxNumUrl:\n",
        "    # Dequeue a URL from urls and try to open and read it\n",
        "    try:\n",
        "        curr_url = urls.pop(0)\n",
        "        req = urllib.request.Request(curr_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "        webpage = urllib.request.urlopen(req).read()\n",
        "\n",
        "    except Exception as ex:\n",
        "        continue\n",
        "\n",
        "    # If URL opens, check if it's a press release page and contains \"crisis\"\n",
        "    soup = BeautifulSoup(webpage, 'html.parser')\n",
        "\n",
        "    if is_press(soup) and \"crisis\" in soup.get_text().lower():\n",
        "        press_releases.append(curr_url)\n",
        "\n",
        "    # Extract links\n",
        "    for tag in soup.find_all('a', href=True):\n",
        "        child_url = tag['href']\n",
        "        child_url = urllib.parse.urljoin(seed_url, child_url)\n",
        "        if seed_url in child_url and child_url not in urls:\n",
        "            urls.append(child_url)\n",
        "\n",
        "\n",
        "for press_release_url in press_releases:\n",
        "    print(press_release_url)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEfuVkU3CuIO",
        "outputId": "e7cbc991-4598-452a-f794-77fbeb2dd051"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting with url=['https://press.un.org/en']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-ce847b44527a>:6: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
            "  press_release_link = soup.find('a', href=lambda href: href and href.startswith(\"/en/press-release\"), hreflang=\"en\", text=\"Press Release\")\n",
            "/usr/local/lib/python3.10/dist-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://press.un.org/en/2023/sgsm21967.doc.htm\n",
            "https://press.un.org/en/2023/sgsm21947.doc.htm\n",
            "https://press.un.org/en/2023/dsgsm1874.doc.htm\n",
            "https://press.un.org/en/2023/sgsm21952.doc.htm\n",
            "https://press.un.org/en/2023/sgsm21876.doc.htm\n",
            "https://press.un.org/en/2023/sgsm21852.doc.htm\n",
            "https://press.un.org/en/2023/sgsm21806.doc.htm\n",
            "https://press.un.org/en/2023/dsgsm1848.doc.htm\n",
            "https://press.un.org/en/2023/sgsm21765.doc.htm\n",
            "https://press.un.org/en/2023/sgsm21767.doc.htm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 2"
      ],
      "metadata": {
        "id": "e3ErCknzERoq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import urllib.request\n",
        "\n",
        "# Check if the article is a plenary session\n",
        "def is_plenary(soup2):\n",
        "    plenary_session_text = soup2.find('span', class_='ep_name', text=\"Plenary session\")\n",
        "    return plenary_session_text is not None\n",
        "\n",
        "seed_url = \"https://www.europarl.europa.eu/news/en/press-room\"\n",
        "\n",
        "urls = [seed_url]  # Queue of URLs to crawl\n",
        "press_releases = [] # Store found press releases\n",
        "\n",
        "maxNumUrl = 10  # Set the maximum number of URLs to visit\n",
        "\n",
        "print(\"Starting with url=\" + str(urls))\n",
        "while urls and len(press_releases) < maxNumUrl:\n",
        "    # Dequeue a URL from urls and try to open and read it\n",
        "    try:\n",
        "        curr_url = urls.pop(0)\n",
        "        req = urllib.request.Request(curr_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "        webpage = urllib.request.urlopen(req).read()\n",
        "\n",
        "    except Exception as ex:\n",
        "        continue\n",
        "\n",
        "    # If URL opens, check if it's a press release page and contains \"crisis\"\n",
        "    soup = BeautifulSoup(webpage, 'html.parser')\n",
        "\n",
        "    if is_plenary(soup) and \"crisis\" in soup.get_text().lower():\n",
        "        press_releases.append(curr_url)\n",
        "\n",
        "    # Extract links\n",
        "    for tag in soup.find_all('a', href=True):\n",
        "        child_url = tag['href']\n",
        "        child_url = urllib.parse.urljoin(seed_url, child_url)\n",
        "        if seed_url in child_url and child_url not in urls:\n",
        "            urls.append(child_url)\n",
        "\n",
        "\n",
        "for press_release_url in press_releases:\n",
        "    print(press_release_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRwdfl7AUHuQ",
        "outputId": "90c78de1-0fc8-423b-b8dd-8b0451d043d5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting with url=['https://www.europarl.europa.eu/news/en/press-room']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-c77db218eabf>:6: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
            "  plenary_session_text = soup2.find('span', class_='ep_name', text=\"Plenary session\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://www.europarl.europa.eu/news/en/press-room/20230929IPR06132/nagorno-karabakh-meps-demand-review-of-eu-relations-with-azerbaijan\n",
            "https://www.europarl.europa.eu/news/en/press-room?contentType=plenary\n",
            "https://www.europarl.europa.eu/news/en/press-room?contentType=committee&keywordValue=ECON\n",
            "https://www.europarl.europa.eu/news/en/press-room/20230929IPR06132/nagorno-karabakh-meps-demand-review-of-eu-relations-with-azerbaijan\n",
            "https://www.europarl.europa.eu/news/en/press-room?contentType=committee&keywordValue=AFET\n",
            "https://www.europarl.europa.eu/news/en/press-room?contentType=committee&keywordValue=PETI\n",
            "https://www.europarl.europa.eu/news/en/press-room/20221209IPR64426/eu-long-term-budget-needs-urgent-revision-to-cope-with-current-crises\n",
            "https://www.europarl.europa.eu/news/en/press-room/20210304IPR99207/parliament-gives-green-light-for-new-eu4health-programme\n",
            "https://www.europarl.europa.eu/news/en/press-room/20220909IPR40138/parliament-adopts-new-rules-on-adequate-minimum-wages-for-all-workers-in-the-eu\n",
            "https://www.europarl.europa.eu/news/en/press-room/20230310IPR77232/minimum-income-schemes-increasing-support-accessibility-and-inclusion\n"
          ]
        }
      ]
    }
  ]
}